{"cells":[{"cell_type":"code","execution_count":1,"id":"4392baa5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","\n","import json\n","import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *\n","\n"]},{"cell_type":"code","execution_count":5,"id":"2a7bd087","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["bucket_name = 'irproject_bucket' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh' and ('/' not in b.name or b.name.endswith('/')):\n","        paths.append(full_path+b.name)\n","\n","parquetFile = spark.read.parquet(*paths)\n","\n","wiki1000_body = parquetFile.select(\"id\", \"text\").rdd\n","wiki1000_title = parquetFile.select(\"id\", \"title\").rdd\n","wiki1000_anchor = parquetFile.select(\"id\", \"anchor_text\").rdd"]},{"cell_type":"markdown","id":"f1643f23","metadata":{},"source":["ID TO TITLES"]},{"cell_type":"code","execution_count":6,"id":"2ae6154c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["ids_titles = wiki1000_title.collectAsMap()"]},{"cell_type":"code","execution_count":7,"id":"776b4e9e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://titles.json [Content-Type=application/json]...\n","==> NOTE: You are uploading one or more large file(s), which would run          \n","significantly faster if you enable parallel composite uploads. This\n","feature can be enabled by editing the\n","\"parallel_composite_upload_threshold\" value in your .boto\n","configuration file. However, note that if you do this large files will\n","be uploaded as `composite objects\n","<https://cloud.google.com/storage/docs/composite-objects>`_,which\n","means that any user who downloads such objects will need to have a\n","compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n","without a compiled crcmod, computing checksums on composite objects is\n","so slow that gsutil disables downloads of composite objects.\n","\n","\\ [1 files][218.2 MiB/218.2 MiB]                                                \n","Operation completed over 1 objects/218.2 MiB.                                    \n"]}],"source":["with open('titles.json', 'w') as titles:\n","     json.dump(ids_titles, titles)\n","\n","titles_src = \"titles.json\"\n","titles_dst = f'gs://{bucket_name}/titles/{titles_src}'\n","!gsutil cp $titles_src $titles_dst"]},{"cell_type":"code","execution_count":8,"id":"e24b6b60","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index.py\r\n"]}],"source":["%cd -q /home/dataproc\n","!ls inverted_index.py\n","sc.addFile(\"/home/dataproc/inverted_index.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())\n","from inverted_index import InvertedIndex"]},{"cell_type":"code","execution_count":9,"id":"1284b91c","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","ps = PorterStemmer()"]},{"cell_type":"code","execution_count":10,"id":"64ff4d8a","metadata":{},"outputs":[],"source":["def get_tf(doc_id, text):\n","    # Tokenize, filter, and count in one step for efficiency\n","    tokens = (token.group() for token in RE_WORD.finditer(text.lower()))\n","    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n","    \n","    # Count tokens and prepare output format\n","    token_counts = Counter(filtered_tokens)\n","    return [(token, (doc_id, count)) for token, count in token_counts.items()]"]},{"cell_type":"code","execution_count":16,"id":"718750a9","metadata":{},"outputs":[],"source":["def get_DL(text):\n","    # Tokenize and filter using a generator expression for efficiency\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    filtered_tokens = [token for token in tokens if token not in all_stopwords]\n","    \n","    # Count the number of filtered tokens directly\n","    return len(filtered_tokens)"]},{"cell_type":"markdown","id":"a669e398","metadata":{},"source":["BODY"]},{"cell_type":"code","execution_count":12,"id":"539a23e9","metadata":{},"outputs":[],"source":["#get tfs\n","body_tfs = wiki1000_body.flatMap(lambda x: get_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":13,"id":"4fcb1be1","metadata":{},"outputs":[],"source":["#sort pls\n","body_postings = body_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)\n","body_postings_filtered = body_postings.filter(lambda x: len(x[1])>50)"]},{"cell_type":"code","execution_count":14,"id":"8c8bc546","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_df = InvertedIndex.calculate_df(body_postings_filtered)\n","body_df_dict = body_df.collectAsMap()"]},{"cell_type":"code","execution_count":17,"id":"8102bf5d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_DL = wiki1000_body.map(lambda x: (x[0], get_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":18,"id":"57394c63","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_tot_term = body_postings_filtered.mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":19,"id":"351444d8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["body_posting_locs_list = InvertedIndex.partition_postings_and_write(body_postings_filtered, bucket_name, \"body\").collect()"]},{"cell_type":"code","execution_count":20,"id":"74616980","metadata":{},"outputs":[],"source":["body_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='body'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            body_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":21,"id":"51980216","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://body_index.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 69.0 MiB/ 69.0 MiB]                                                \n","Operation completed over 1 objects/69.0 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","body_inv_index = InvertedIndex()\n","body_inv_index.DL = body_DL\n","\n","# Adding the posting locations dictionary to the inverted index\n","body_inv_index.posting_locs = body_super_posting_locs\n","\n","# Add the token - df dictionary to the inverted index\n","body_inv_index.df = body_df_dict\n","body_inv_index.term_total = body_tot_term\n","\n","# write the global stats out\n","body_inv_index.write_index('.', 'body_index')\n","index_src = \"body_index.pkl\"\n","index_dst = f'gs://{bucket_name}/body/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"78bd8cbb","metadata":{},"outputs":[],"source":["# body_inv_index.read_posting_list('state', 'body', bucket_name)"]},{"cell_type":"markdown","id":"47801f3c","metadata":{},"source":["TITLE"]},{"cell_type":"code","execution_count":22,"id":"6d2e01c2","metadata":{},"outputs":[],"source":["def get_title_tf(id, text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    # use counter to count and map values with id \n","    stems = []\n","    for token in tokens:\n","        stems.append(ps.stem(token))\n","    return list(map(lambda x: (x[0], (id, x[1])), list(Counter(stems).items())))\n","\n","def get_title_DL(text):\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    stems = []\n","    for token in tokens:\n","        stems.append(ps.stem(token))\n","    return len(stems)\n"]},{"cell_type":"code","execution_count":23,"id":"e6cb106f","metadata":{},"outputs":[],"source":["#get tfs\n","title_tfs = wiki1000_title.flatMap(lambda x: get_title_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":24,"id":"c1aea495","metadata":{},"outputs":[],"source":["# #sort pls\n","title_postings = title_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)"]},{"cell_type":"code","execution_count":25,"id":"04dade65","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_df = InvertedIndex.calculate_df(title_postings)\n","title_df_dict = title_df.collectAsMap()"]},{"cell_type":"code","execution_count":26,"id":"3dbafe7d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_DL = wiki1000_title.map(lambda x: (x[0], get_title_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":27,"id":"0dac7b85","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_tot_term = title_tfs.groupByKey().mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":28,"id":"6cccb357","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["title_posting_locs_list = InvertedIndex.partition_postings_and_write(title_postings, bucket_name, \"title\").collect()"]},{"cell_type":"code","execution_count":29,"id":"4b6a1b2b","metadata":{},"outputs":[],"source":["title_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='title'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            title_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":30,"id":"cf41d5a2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","| [1 files][126.1 MiB/126.1 MiB]                                                \n","Operation completed over 1 objects/126.1 MiB.                                    \n"]}],"source":["# Create inverted index instance\n","title_inv_index = InvertedIndex()\n","title_inv_index.DL = title_DL\n","\n","# Adding the posting locations dictionary to the inverted index\n","title_inv_index.posting_locs = title_super_posting_locs\n","\n","# Add the token - df dictionary to the inverted index\n","title_inv_index.df = title_df_dict\n","title_inv_index.term_total = title_tot_term\n","\n","# write the global stats out\n","title_inv_index.write_index('.', 'title_index')\n","index_src2 = \"title_index.pkl\"\n","index_dst2 = f'gs://{bucket_name}/title/{index_src2}'\n","!gsutil cp $index_src2 $index_dst2"]},{"cell_type":"code","execution_count":31,"id":"46d310ef","metadata":{},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["title_inv_index.read_posting_list('anarchism', 'title', bucket_name)"]},{"cell_type":"code","execution_count":32,"id":"0e50db6b","metadata":{},"outputs":[{"data":{"text/plain":["[(12, 1),\n"," (14936, 1),\n"," (98514, 1),\n"," (105859, 1),\n"," (371351, 1),\n"," (470052, 1),\n"," (673063, 1),\n"," (805586, 1),\n"," (1063286, 1),\n"," (1249918, 1),\n"," (1325940, 1),\n"," (1332770, 1),\n"," (1433310, 1),\n"," (1596739, 1),\n"," (1596742, 1),\n"," (2052697, 1),\n"," (2141543, 1),\n"," (2274182, 1),\n"," (2287742, 1),\n"," (2382358, 1),\n"," (2553405, 1),\n"," (2722899, 1),\n"," (4117528, 1),\n"," (4398733, 1),\n"," (4532867, 1),\n"," (4687957, 1),\n"," (4977561, 1),\n"," (5601442, 1),\n"," (5658365, 1),\n"," (5773736, 1),\n"," (5879835, 1),\n"," (6244745, 1),\n"," (6759361, 1),\n"," (7066267, 1),\n"," (7080268, 1),\n"," (7475769, 1),\n"," (8360302, 1),\n"," (8364409, 1),\n"," (8603686, 1),\n"," (8784176, 1),\n"," (8817545, 1),\n"," (8874671, 1),\n"," (9265113, 1),\n"," (10304567, 1),\n"," (10403320, 1),\n"," (11630863, 1),\n"," (11893205, 1),\n"," (14526954, 1),\n"," (14603019, 1),\n"," (14728186, 1),\n"," (14946461, 1),\n"," (15280013, 1),\n"," (15475159, 1),\n"," (15481723, 1),\n"," (15484912, 1),\n"," (15499386, 1),\n"," (16032615, 1),\n"," (16102540, 1),\n"," (17846785, 1),\n"," (17878864, 1),\n"," (18017282, 1),\n"," (18328984, 1),\n"," (18330784, 1),\n"," (18809819, 1),\n"," (20264216, 1),\n"," (20679746, 1),\n"," (20684316, 1),\n"," (22071472, 1),\n"," (22882894, 1),\n"," (24114294, 1),\n"," (24957712, 1),\n"," (25219795, 1),\n"," (25848775, 1),\n"," (25934294, 1),\n"," (26271818, 1),\n"," (26729461, 1),\n"," (30418740, 1),\n"," (31619520, 1),\n"," (32879153, 1),\n"," (33287912, 1),\n"," (34810484, 1),\n"," (35968770, 1),\n"," (37727991, 1),\n"," (38940391, 1),\n"," (39353100, 1),\n"," (39398243, 1),\n"," (40447618, 1),\n"," (40447622, 1),\n"," (40678640, 1),\n"," (41292335, 1),\n"," (43356519, 1),\n"," (43701697, 1),\n"," (43706082, 1),\n"," (43714500, 1),\n"," (43760019, 1),\n"," (43823995, 1),\n"," (44065958, 1),\n"," (46399294, 1),\n"," (48429575, 1),\n"," (50496086, 1),\n"," (50930644, 1),\n"," (50942388, 1),\n"," (52430227, 1),\n"," (56799909, 1),\n"," (57587432, 1),\n"," (58836108, 1),\n"," (58836779, 1),\n"," (59043122, 1),\n"," (59682840, 1),\n"," (59938989, 1),\n"," (60255933, 1),\n"," (60552489, 1),\n"," (60912661, 1),\n"," (61411509, 1),\n"," (62590370, 1),\n"," (62591673, 1),\n"," (62821053, 1),\n"," (63657257, 1),\n"," (64505320, 1),\n"," (64521811, 1),\n"," (64829376, 1),\n"," (65372098, 1),\n"," (65697081, 1),\n"," (65752143, 1),\n"," (65918497, 1),\n"," (66220139, 1),\n"," (66225218, 1),\n"," (66229782, 1),\n"," (66231347, 1),\n"," (66231929, 1),\n"," (66279030, 1),\n"," (66340836, 1),\n"," (66342996, 1),\n"," (66343664, 1),\n"," (66345512, 1),\n"," (66346084, 1),\n"," (66416177, 1),\n"," (66478766, 1),\n"," (66508948, 1),\n"," (66615939, 1),\n"," (66649227, 1),\n"," (66687783, 1),\n"," (66715293, 1),\n"," (66797129, 1),\n"," (66812726, 1),\n"," (66822481, 1),\n"," (66847695, 1),\n"," (66911393, 1),\n"," (66978065, 1),\n"," (66994147, 1),\n"," (67011738, 1),\n"," (67029163, 1),\n"," (67154655, 1),\n"," (67168825, 1),\n"," (67218991, 1),\n"," (67772793, 1),\n"," (68304176, 1),\n"," (68304182, 1),\n"," (68366104, 1)]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["title_inv_index.read_posting_list('anarch', 'title', bucket_name)"]},{"cell_type":"markdown","id":"b5ec6a46","metadata":{},"source":["ANCHOR"]},{"cell_type":"code","execution_count":33,"id":"6cee4df5","metadata":{},"outputs":[],"source":["def get_anchor_tf(id, rows):\n","    rows2 = list(map(lambda x: x[1], rows))\n","    text = ''\n","    for t in rows2:\n","        text+=t+\" \"\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    # use counter to count and map values with id \n","    return list(map(lambda x: (x[0], (id, x[1])), list(Counter(tokens).items())))"]},{"cell_type":"code","execution_count":34,"id":"d5e60e25","metadata":{},"outputs":[],"source":["def get_anchor_DL(rows):\n","    rows2 = list(map(lambda x: x[1], rows))\n","    text = ''\n","    for t in rows2:\n","        text+=t+\" \"\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    # filtered tokens\n","    tokens = [token for token in tokens if token not in all_stopwords]\n","    return len(tokens) # TODO Maybe not filtered"]},{"cell_type":"code","execution_count":35,"id":"2e8ce9e1","metadata":{},"outputs":[],"source":["#get tfs\n","anchor_tfs = wiki1000_anchor.flatMap(lambda x: get_anchor_tf(x[0], x[1]))"]},{"cell_type":"code","execution_count":36,"id":"35c3dc9a","metadata":{},"outputs":[],"source":["# sort pls\n","anchor_postings = anchor_tfs.groupByKey().mapValues(InvertedIndex.reduce_word_counts)\n","anchor_postings_filtered = body_postings.filter(lambda x: len(x[1])>20)"]},{"cell_type":"code","execution_count":37,"id":"1c16375d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_df = InvertedIndex.calculate_df(anchor_postings_filtered)\n","anchor_df_dict = anchor_df.collectAsMap()"]},{"cell_type":"code","execution_count":38,"id":"46100c9d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_DL = wiki1000_anchor.map(lambda x: (x[0], get_anchor_DL(x[1]))).collectAsMap()"]},{"cell_type":"code","execution_count":null,"id":"b2de3b6c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 26:====================================================> (121 + 3) / 124]\r"]}],"source":["anchor_tot_term = anchor_postings_filtered.mapValues(InvertedIndex.get_total_term).collectAsMap()"]},{"cell_type":"code","execution_count":27,"id":"4b7b1e21","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["anchor_posting_locs_list = InvertedIndex.partition_postings_and_write(anchor_postings_filtered, bucket_name, \"anchor\").collect()"]},{"cell_type":"code","execution_count":28,"id":"05b6581d","metadata":{},"outputs":[],"source":["anchor_super_posting_locs = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix='anchor'):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            anchor_super_posting_locs[k].extend(v)"]},{"cell_type":"code","execution_count":30,"id":"adedf085","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://anchor_index.pkl [Content-Type=application/octet-stream]...\n","- [1 files][ 91.4 MiB/ 91.4 MiB]                                                \n","Operation completed over 1 objects/91.4 MiB.                                     \n"]}],"source":["# Create inverted index instance\n","anchor_inv_index = InvertedIndex()\n","anchor_inv_index.DL = anchor_DL\n","\n","# Adding the posting locations dictionary to the inverted index\n","anchor_inv_index.posting_locs = anchor_super_posting_locs\n","\n","# Add the token - df dictionary to the inverted index\n","anchor_inv_index.df = anchor_df_dict\n","anchor_inv_index.term_total = anchor_tot_term\n","\n","# write the global stats out\n","anchor_inv_index.write_index('.', 'anchor_index')\n","index_src3 = \"anchor_index.pkl\"\n","index_dst3 = f'gs://{bucket_name}/anchor/{index_src3}'\n","!gsutil cp $index_src3 $index_dst3"]},{"cell_type":"code","execution_count":null,"id":"0b850976","metadata":{},"outputs":[],"source":["# anchor_inv_index.read_posting_list('teresa', 'anchor', bucket_name)"]},{"cell_type":"markdown","id":"8c197396","metadata":{},"source":["PAGE RANK"]},{"cell_type":"code","execution_count":null,"id":"3d0bbb00","metadata":{},"outputs":[],"source":["def get_ids_from_anchors(id,anchorlist):\n","    return [(id, anchor[0]) for anchor in anchorlist]\n","  \n","\n","def generate_graph(pages):\n","    # Flatten the list of anchor IDs for vertices and create distinct vertices\n","    verticesFromLinks = pages.flatMap(lambda x: [y[0] for y in x[1]]).distinct()\n","    verticeFromIds = pages.map(lambda x: x[0]).distinct()\n","\n","    # Union of vertices from IDs and links ensures all unique vertices are considered\n","    vertices = verticesFromLinks.union(verticeFromIds).map(lambda x: (x, ))\n","\n","    # Generate edges by flattening the anchor list with the corresponding page ID\n","    edges = pages.flatMap(lambda x: get_ids_from_anchors(x[0], x[1])).distinct()\n","\n","    return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"4b5de070","metadata":{},"outputs":[],"source":["edges, vertices = generate_graph(wiki1000_anchor)\n","v_cnt, e_cnt = vertices.count(), edges.count()"]},{"cell_type":"code","execution_count":null,"id":"ead28624","metadata":{},"outputs":[],"source":["edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","\n","dictpr = {}\n","for row in pr.toPandas().iterrows():\n","    dictpr[int(row[1][0])]=row[1][1]\n","\n","with open('pr.json', 'w') as pr:\n","     json.dump(dictpr, pr)\n","\n","pr_src = \"pr.json\"\n","pr_dst = f'gs://{bucket_name}/pr/{pr_src}'\n","!gsutil cp $pr_src $pr_dst"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
